prompts:
  - "{{prompt}}"

providers:
  - id: file://./dist/providers/convex-agent.js
    label: convex-agent

tests: file://./cases/fill-blanks.yaml

# Custom grader using Vercel AI Gateway
defaultTest:
  assert:
    - type: llm-rubric
      value: |
        Evaluate this educational exercise output. Score each dimension 0.0-1.0:

        1. STRUCTURAL VALIDITY: Is the XML well-formed with proper <exercise> container, correct element nesting, and required attributes (like student-answer="" on blanks)?

        2. DIFFICULTY MATCH: Is the difficulty appropriate for the stated CEFR level? Consider vocabulary complexity, grammar structures, and cognitive demand.

        3. INSTRUCTION CLARITY: Are the exercise instructions clear and unambiguous? Would a student know exactly what to do?

        4. LEARNING OBJECTIVE ALIGNMENT: Does the exercise effectively test the skill or concept mentioned in the instruction?

        Provide reasoning for each dimension BEFORE giving the score.
        Output as JSON: { "reasoning": { "structural_validity": "...", "difficulty_match": "...", "instruction_clarity": "...", "learning_objective_alignment": "..." }, "scores": { "structural_validity": 0.0, "difficulty_match": 0.0, "instruction_clarity": 0.0, "learning_objective_alignment": 0.0 } }
      threshold: 0.70
      provider: file://./dist/providers/vercel-ai-grader.js

outputPath: ./results/latest.json
