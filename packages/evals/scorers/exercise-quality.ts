import { anthropic } from "@ai-sdk/anthropic";
import { generateObject } from "ai";
import { z } from "zod";

export interface DimensionScore {
  score: number;
  reasoning: string;
}

export interface ScoreResult {
  dimensions: {
    structural_validity: DimensionScore;
    difficulty_match: DimensionScore;
    instruction_clarity: DimensionScore;
    learning_objective_alignment: DimensionScore;
  };
  overall: number;
  passed: boolean;
}

export interface ScoreInput {
  agentOutput: string;
  documentXml: string;
  instruction: string;
  cefrLevel?: string;
  exerciseType?: string;
}

export interface ScorerConfig {
  threshold: number;
}

interface StructuralCheckResult {
  passed: number;
  total: number;
}

function checkExerciseContainer(output: string): boolean {
  return output.includes("<exercise") && output.includes("</exercise>");
}

function checkFillBlanks(output: string): boolean {
  return (
    output.includes("<blank") &&
    output.includes("answer=") &&
    output.includes('student-answer=""')
  );
}

function checkMultipleChoice(output: string): boolean {
  const hasOl = output.includes("<ol");
  const liMatches = output.match(/<li[\s>]/g);
  return hasOl && liMatches !== null && liMatches.length >= 4;
}

function checkTrueFalse(output: string): boolean {
  const hasParagraphs = output.includes("<p>");
  const hasWritingAreas = output.includes("<writing-area");
  return hasParagraphs || hasWritingAreas;
}

function checkSequencing(output: string): boolean {
  const hasNumberedItems = /\d+[.)]\s/.test(output);
  const hasWritingAreas = output.includes("<writing-area");
  return hasNumberedItems || hasWritingAreas;
}

function checkWriting(output: string): boolean {
  return output.includes("<writing-area") && output.includes("lines=");
}

function runTypeSpecificChecks(
  output: string,
  exerciseType: string | undefined,
): StructuralCheckResult {
  if (!exerciseType) {
    return { passed: 1, total: 1 };
  }

  const checks: boolean[] = [];

  switch (exerciseType) {
    case "fill-blanks":
      checks.push(checkFillBlanks(output));
      break;
    case "multiple-choice":
      checks.push(checkMultipleChoice(output));
      break;
    case "true-false":
      checks.push(checkTrueFalse(output));
      break;
    case "sequencing":
      checks.push(checkSequencing(output));
      break;
    case "short-answer":
    case "summary-writing":
    case "opinion-writing":
    case "discussion-prompt":
    case "description-writing":
    case "dictation":
    case "role-play":
    case "sentence-completion":
      checks.push(checkWriting(output));
      break;
    default:
      break;
  }

  return {
    passed: checks.filter(Boolean).length,
    total: checks.length,
  };
}

function checkStructuralValidity(input: ScoreInput): DimensionScore {
  const hasContainer = checkExerciseContainer(input.agentOutput);

  if (!hasContainer) {
    return {
      score: 0.0,
      reasoning:
        "Output is missing the required <exercise> container tags. " +
        "A valid exercise must be wrapped in <exercise ...>...</exercise>.",
    };
  }

  const typeChecks = runTypeSpecificChecks(
    input.agentOutput,
    input.exerciseType,
  );

  const totalChecks = 1 + typeChecks.total;
  const passedChecks = 1 + typeChecks.passed;
  const ratio = passedChecks / totalChecks;

  let score: number;
  if (ratio === 1) {
    score = 1.0;
  } else if (ratio > 0) {
    score = 0.5;
  } else {
    score = 0.0;
  }

  const typeLabel = input.exerciseType ?? "unspecified";
  const reasoning =
    `Exercise container: present. ` +
    `Type-specific checks (${typeLabel}): ${typeChecks.passed}/${typeChecks.total} passed. ` +
    (score === 1.0
      ? "All structural requirements met."
      : score === 0.5
        ? "Some structural requirements missing â€” review exercise format."
        : "Critical structural elements missing.");

  return { score, reasoning };
}

const judgeSchema = z.object({
  difficulty_match: z.object({
    reasoning: z.string(),
    score: z.number().min(0).max(1),
  }),
  instruction_clarity: z.object({
    reasoning: z.string(),
    score: z.number().min(0).max(1),
  }),
  learning_objective_alignment: z.object({
    reasoning: z.string(),
    score: z.number().min(0).max(1),
  }),
});

type JudgeOutput = z.infer<typeof judgeSchema>;

function buildJudgePrompt(input: ScoreInput): string {
  const cefrLabel = input.cefrLevel ?? "not specified";
  const typeLabel = input.exerciseType ?? "not specified";

  return [
    "You are evaluating an educational exercise generated by an AI assistant.",
    "",
    "CONTEXT:",
    `- Original document: ${input.documentXml}`,
    `- Teacher's instruction: ${input.instruction}`,
    `- Expected CEFR level: ${cefrLabel}`,
    `- Expected exercise type: ${typeLabel}`,
    "",
    "GENERATED EXERCISE OUTPUT:",
    input.agentOutput,
    "",
    "For each dimension, provide reasoning FIRST, then a score 0.0-1.0:",
    "",
    "1. DIFFICULTY MATCH (0.0-1.0):",
    "   - 0.0: Completely wrong level (college content for beginners)",
    "   - 0.5: Somewhat appropriate but could be better calibrated",
    "   - 1.0: Precisely calibrated for the stated CEFR level",
    "",
    "2. INSTRUCTION CLARITY (0.0-1.0):",
    "   - 0.0: Instructions are missing or completely confusing",
    "   - 0.5: Instructions exist but are ambiguous",
    "   - 1.0: Instructions are crystal clear, student knows exactly what to do",
    "",
    "3. LEARNING OBJECTIVE ALIGNMENT (0.0-1.0):",
    "   - 0.0: Exercise tests something completely different from what was asked",
    "   - 0.5: Partially aligned but misses key aspects",
    "   - 1.0: Exercise precisely tests the skill/concept from the instruction",
  ].join("\n");
}

async function runLLMJudge(input: ScoreInput): Promise<JudgeOutput> {
  const { object } = await generateObject({
    model: anthropic("claude-sonnet-4-5"),
    schema: judgeSchema,
    prompt: buildJudgePrompt(input),
  });

  return object;
}

const DEFAULT_THRESHOLD = 0.7;

export async function scoreExercise(
  input: ScoreInput,
  config: ScorerConfig = { threshold: DEFAULT_THRESHOLD },
): Promise<ScoreResult> {
  const structuralValidity = checkStructuralValidity(input);
  const llmScores = await runLLMJudge(input);

  const dimensions = {
    structural_validity: structuralValidity,
    difficulty_match: {
      score: llmScores.difficulty_match.score,
      reasoning: llmScores.difficulty_match.reasoning,
    },
    instruction_clarity: {
      score: llmScores.instruction_clarity.score,
      reasoning: llmScores.instruction_clarity.reasoning,
    },
    learning_objective_alignment: {
      score: llmScores.learning_objective_alignment.score,
      reasoning: llmScores.learning_objective_alignment.reasoning,
    },
  };

  const scores = Object.values(dimensions).map((d) => d.score);
  const overall = scores.reduce((sum, s) => sum + s, 0) / scores.length;
  const passed = scores.every((s) => s >= config.threshold);

  return { dimensions, overall, passed };
}
